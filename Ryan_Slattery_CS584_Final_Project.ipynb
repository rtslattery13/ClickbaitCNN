{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab21296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RYAN SLATTERY\n",
    "# CS584 - NATURAL LANGUAGE PROCESSING\n",
    "# START DATE: MARCH 21, 2022\n",
    "# LAST MODIFIED: MAY 06, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING AND SHUFFLING OUR DATA:\n",
    "# Since the Kaggle Dataset is organized by class (the first half of the dataset is all class 1, and the second half is\n",
    "# all class 0), I decided to shuffle the data.\n",
    "data = pd.read_csv(\"clickbait_data.csv\")\n",
    "data = data.sample(frac = 1)\n",
    "\n",
    "headlines = data['headline'].values.tolist()\n",
    "labels = data['clickbait'].values.tolist()\n",
    "\n",
    "# SEPARATING OUR TESTING/TRAINING DATA:\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(headlines, labels, test_size = 0.1)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size = 0.1)\n",
    "\n",
    "print(\"THE SIZE OF THE TRAINING SET IS:\", len(train_texts))\n",
    "print(\"THE SIZE OF THE VALIDATION SET IS:\", len(valid_texts))\n",
    "print(\"THE SIZE OF THE TESTING SET IS:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# FEATURE EXTRACTION:\n",
    "class TF_IDF(object):\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tf = defaultdict(lambda: 0)\n",
    "        self.df = defaultdict(lambda: 0)\n",
    "        self.word2idx = {}\n",
    "        \n",
    "    # PREPROCESSING METHOD:\n",
    "    # INPUT: text (a string of text)\n",
    "    # OUTPUT: text (a cleaned version of the input text)\n",
    "    # A simple preprocessor function that replaces uppercase letters with lowercase ones, and removes punctuation, \n",
    "    # numbers, and urls (if there are any... it is unlikely since the text only consists of headlines).\n",
    "    def Preprocessor(self, text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[0-9]+', '', text)\n",
    "        text = text.lower()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    # TOKENIZER METHOD:\n",
    "    # INPUT: text (a string of text)\n",
    "    # OUTPUT: tokens (a list of tokens)\n",
    "    def Tokenizer(self, text):\n",
    "        docs = nlp.tokenizer.pipe(text, batch_size = 1000)\n",
    "        tokens = []\n",
    "\n",
    "        for doc in docs:\n",
    "            for token in doc:\n",
    "                if token.is_stop == False:\n",
    "                    tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    # COUNT STOP WORDS METHOD:\n",
    "    # INPUT: text (a string of text)\n",
    "    # OUTPUT: len(stops) (number of stopwords within the input text)\n",
    "    def CountStopWords(self, text):\n",
    "        docs = nlp.tokenizer.pipe(text, batch_size = 1000)\n",
    "        stops = 0\n",
    "        length = 0\n",
    "        \n",
    "        for doc in docs:\n",
    "            stops = [token for token in doc if token.is_stop == True]\n",
    "            length = [token for token in doc]\n",
    "\n",
    "        return len(stops), len(stops)/len(length)\n",
    "    \n",
    "    # PRONOUN SCORE METHOD:\n",
    "    # INPUT: text (a string of text)\n",
    "    # OUTPUT: score (number of pronouns over length of text)\n",
    "    def PronounScore(self, text):\n",
    "        pronouns = [\"i\", \"you\", \"we'll\", \"he\", \"she\", \"him\", \"her\", \"you'll\", \"your\", \"his\", \"hers\", \"they\", \"them\", \"theirs\", \"me\", \"mine\", \"it\", \"yours\", \"we\", \"their\", \"my\"]\n",
    "        text = text.lower()\n",
    "        text = text.split()\n",
    "        \n",
    "        pros = [word for word in text if word in pronouns]\n",
    "        score = len(pros)/len(text)\n",
    "        \n",
    "        return len(pros), score\n",
    "        \n",
    " \n",
    "    # FIT METHOD:\n",
    "    # INPUT: texts (a list of texts)\n",
    "    # Calls on the Preprocessor and Tokenizer to clean and tokenize the text. Takes the resulting tokens and \n",
    "    # stores them within the tf and df dictionaries initialized in the constructor.\n",
    "    def Fit(self, texts):\n",
    "        num_text = 0\n",
    "        self.tf['<PAD>'] = 0\n",
    "        self.df['<PAD>'] = 0\n",
    "        \n",
    "        for text in tqdm(texts, desc = 'FITTING TEXTS'):\n",
    "            clean_text = self.Preprocessor(text)\n",
    "            tokens = self.Tokenizer([clean_text])\n",
    "            \n",
    "            for token in tokens:\n",
    "                self.df[token.text_with_ws] += 1\n",
    "                \n",
    "                if (token.text_with_ws not in self.tf):\n",
    "                    self.tf[token.text_with_ws] = [num_text]\n",
    "                else:\n",
    "                    self.tf[token.text_with_ws].append(num_text)\n",
    "            \n",
    "            num_text += 1\n",
    "        \n",
    "        \n",
    "        if self.tf is not None:\n",
    "            self.tf = {key: self.tf[key] for key in list(self.tf.keys())[:self.vocab_size]}\n",
    "        \n",
    "        self.word2idx = {key: idx for idx, key in enumerate(self.tf.keys())}\n",
    "\n",
    "    \n",
    "    # TRANSFORM METHOD:\n",
    "    # INPUT: texts (list of texts), sequence_length (defined length of sequences)\n",
    "    # OUTPUT: tf_idf (feature vectors), text_index_list (indices for each word in the vocabulary, for each doc), \n",
    "    # num_stop_words (number of stop words for each document)\n",
    "    def Transform(self, texts, sequence_length):\n",
    "        tf_idf = np.zeros((len(texts), self.vocab_size))\n",
    "        num_stop_words = np.zeros((len(texts), 1))\n",
    "        num_pronouns = np.zeros((len(texts), 1))\n",
    "        stop_words_ratio = np.zeros((len(texts), 1))\n",
    "        pronoun_score = np.zeros((len(texts), 1))\n",
    "        text_index_list = np.zeros((len(texts), sequence_length))\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), desc = 'TRANSFORMING', total = len(texts)):\n",
    "            clean_text = self.Preprocessor(text)\n",
    "            tokens = self.Tokenizer([clean_text])\n",
    "            num_stops, stop_ratio = self.CountStopWords([clean_text])\n",
    "            num_stop_words[i] = num_stops\n",
    "            stop_words_ratio[i] = stop_ratio\n",
    "            num_pros, pronoun_score[i] = self.PronounScore(text)\n",
    "            num_pronouns[i] = num_pros\n",
    "        \n",
    "            doc_words = len(tokens)\n",
    "\n",
    "            tf = 0\n",
    "            idf = 0\n",
    "            count = 0\n",
    "            for item in tokens:\n",
    "                if self.tf.get(item.text_with_ws) is not None:\n",
    "                    if (count < sequence_length):\n",
    "                        text_index_list[i][count] = self.word2idx[item.text_with_ws]\n",
    "                        count += 1\n",
    "                    tf_idf[i][self.word2idx[item.text_with_ws]] += (1/doc_words) * np.log((len(texts)/self.df[item.text_with_ws]))\n",
    "        \n",
    "        tf_idf = np.append(tf_idf, stop_words_ratio, axis = 1)\n",
    "        tf_idf = np.append(tf_idf, pronoun_score, axis = 1)\n",
    "        return tf_idf, text_index_list, num_stop_words, num_pronouns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "sequence_length = 10\n",
    "\n",
    "tfidf_extractor = TF_IDF(vocab_size)\n",
    "tfidf_extractor.Fit(train_texts)\n",
    "\n",
    "train_set, train_indices, train_stops, train_pronouns = tfidf_extractor.Transform(train_texts, sequence_length)\n",
    "valid_set, valid_indices, valid_stops, valid_pronouns = tfidf_extractor.Transform(valid_texts, sequence_length)\n",
    "test_set, test_indices, test_stops, test_pronouns = tfidf_extractor.Transform(test_texts, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION OF STOPWORDS BETWEEN CLICKBAIT AND NON-CLICKBAIT TEXTS\n",
    "stops_0 = np.zeros((15, 1))\n",
    "stops_1 = np.zeros((15, 1))\n",
    "\n",
    "for i in range(len(train_stops)):\n",
    "    if (train_labels[i] == 1):\n",
    "        stops_1[int(train_stops[i])] += 1\n",
    "    else:\n",
    "        stops_0[int(train_stops[i])] += 1\n",
    "\n",
    "x = np.arange(15)\n",
    "plt.bar(x-0.2, stops_0.flatten(), 0.4)\n",
    "plt.bar(x+0.2, stops_1.flatten(), 0.4)\n",
    "plt.title('STOPWORDS FOR CLICKBAIT AND NON-CLICKBAIT TITLES')\n",
    "plt.ylabel('NUMBER OF ARTICLES')\n",
    "plt.xlabel('NUMBER OF STOPWORDS')\n",
    "plt.legend(['NON-CLICKBAIT', 'CLICKBAIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8317354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION OF PRONOUNS BETWEEN CLICKBAIT AND NON-CLICKBAIT TEXTS\n",
    "pros_0 = np.zeros((10, 1))\n",
    "pros_1 = np.zeros((10, 1))\n",
    "\n",
    "for i in range(len(train_pronouns)):\n",
    "    if(train_labels[i] == 1):\n",
    "        pros_1[int(train_pronouns[i])] += 1\n",
    "    else:\n",
    "        pros_0[int(train_pronouns[i])] += 1\n",
    "    \n",
    "x = np.arange(10)\n",
    "plt.bar(x-0.2, pros_0.flatten(), 0.4)\n",
    "plt.bar(x+0.2, pros_1.flatten(), 0.4)\n",
    "plt.title('PRONOUNS FOR CLICKBAIT AND NON-CLICKBAIT TITLES')\n",
    "plt.ylabel('NUMBER OF ARTICLES')\n",
    "plt.xlabel('NUMBER OF PRONOUNS')\n",
    "plt.legend(['NON-CLICKBAIT', 'CLICKBAIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "embedding_size = 300\n",
    "hidden_size = 300\n",
    "\n",
    "# HYPERPARAMETERS FOR LOGISTIC REGRESSION:\n",
    "num_epochs_logreg = 25\n",
    "learning_rate_logreg = 0.01\n",
    "\n",
    "# HYPERPARAMETERS FOR CNN:\n",
    "num_epochs_cnn = 10\n",
    "learning_rate_cnn = 0.001\n",
    "num_kernels_cnn = 20\n",
    "\n",
    "# HYPERPARAMETERS FOR CNN W/ EMBEDDINGS:\n",
    "num_epochs_cnn_e = 20\n",
    "learning_rate_cnn_e = 0.025\n",
    "num_kernels_cnn_e = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9313db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC LOGISTIC REGRESSION MODEL:\n",
    "# Contains one output layer, with the result transformed by the sigmoid activation function.\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    # CLASS CONSTRUCTOR:\n",
    "    # INPUT: vocab_size (size of vocabulary)\n",
    "    # NOTE: vocab_size + 2 for input layer due to 2 extra features that were added.\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.output_layer = nn.Linear(vocab_size + 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    # FORWARD PROPOGATION METHOD:\n",
    "    # INPUT: x (input data)\n",
    "    # OUTPUT: y_hat (predicted values between 0 and 1)\n",
    "    # Each batch of inputs get transformed by the linear output layer and the sigmoid activation.\n",
    "    def forward(self, x):\n",
    "        output = self.output_layer(x)\n",
    "        y_hat = torch.squeeze(self.sigmoid(output))\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "# CONVOLUTIONAL NEURAL NETWORK MODEL:\n",
    "# Contains a hidden layer, convolution layer, and output layer from a fully-connected feedforward network.\n",
    "class ClickbaitCNN(nn.Module):\n",
    "    \n",
    "    # CLASS CONSTRUCTOR:\n",
    "    # INPUT: vocab_size (size of vocabulary), hidden_size (size of the hidden layer), num_kernels (number of kernels)\n",
    "    def __init__(self, vocab_size, hidden_size, num_kernels):\n",
    "        super(ClickbaitCNN, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(vocab_size + 2, hidden_size)\n",
    "        self.CNN = nn.Conv1d(hidden_size, num_kernels, kernel_size = 1)\n",
    "        self.pooling = nn.MaxPool1d(num_kernels)\n",
    "        self.output_layer = nn.Linear(1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        \n",
    "    # FORWARD PROPOGATION METHOD:\n",
    "    # INPUT: x (input data)\n",
    "    # OUTPUT: y_hat (predicted values between 0 and 1)\n",
    "    # Each batch of data is transformed by the linear hidden layer, 1D convolution layer, and max-pooled. The processed\n",
    "    # input then gets transformed by another linear output layer and the sigmoid function.\n",
    "    # Slight dropout was included to prevent overfitting.\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.unsqueeze(self.hidden_layer(x), 2))\n",
    "        conv = torch.squeeze(self.CNN(x), 2)\n",
    "        conv = self.pooling(conv)\n",
    "        output = self.output_layer(conv)\n",
    "        y_hat = torch.squeeze(self.sigmoid(output), 1)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "# TRAINED WORD EMBEDDING CONVOLUTIONAL NEURAL NETWORK MODEL:\n",
    "# Contains an embedding layer, convolution layer, and output layer from a fully-connected feedforward network.\n",
    "# The convolution was done with unigrams, as larger order n-grams had issues due to short sequences.\n",
    "class ClickbaitCNN_E(nn.Module):\n",
    "    \n",
    "    # CLASS CONSTRUCTOR:\n",
    "    # INPUT: vocab_size (size of vocabulary), embedding_size (size of the embedding layer), num_kernels (number of kernels)\n",
    "    def __init__(self, vocab_size, embedding_size, num_kernels, sequence_length):\n",
    "        super(ClickbaitCNN_E, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.CNN = nn.Conv1d(embedding_size, 1, kernel_size = 1)\n",
    "        self.pooling = nn.MaxPool1d(sequence_length)\n",
    "        self.output_layer = nn.Linear(1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "    \n",
    "    # FORWARD PROPOGATION METHOD:\n",
    "    # INPUT: x (input data)\n",
    "    # OUTPUT: y_hat (predicted values between 0 and 1)\n",
    "    # Each batch of indices is connected with the embedding layer, 1D convolution layer, and max-pooled. The processed\n",
    "    # input then gets transformed by another linear output layer and the sigmoid function.\n",
    "    # Slight dropout was included to prevent overfitting.\n",
    "    def forward(self, x):\n",
    "        e = self.dropout(torch.transpose(self.embedding_layer(x), 1, 2))\n",
    "        conv = self.CNN(e)\n",
    "        conv = self.pooling(conv)\n",
    "        output = self.output_layer(conv)\n",
    "        y_hat = torch.squeeze(torch.squeeze(self.sigmoid(output), 2), 1)\n",
    "        \n",
    "        return y_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(vocab_size)\n",
    "print(LogReg)\n",
    "\n",
    "CNN = ClickbaitCNN(vocab_size, hidden_size, num_kernels_cnn)\n",
    "print(CNN)\n",
    "\n",
    "CNN_E = ClickbaitCNN_E(vocab_size, embedding_size, num_kernels_cnn_e, sequence_length)\n",
    "print(CNN_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fae0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT TRAINING METHOD:\n",
    "# INPUT: model (model type), training_data (features or embeddings), training_labels (0 and 1), validation_data,\n",
    "# validation_labels, num_epochs (number of epochs **HYPERPARAMETER**), learning_rate (learning rate **HYPERPARAMETER**)\n",
    "# embed (boolean value that converts embedding indices into proper type)\n",
    "# For each epoch, SGD randomly selects instances from the entire training set, feeds them into the model, and calculates\n",
    "# the loss from the output. Binary Cross Entropy Loss is used in this case.\n",
    "# Validation data and labels are passed in as a full batch and are evaluated at the end of each epoch.\n",
    "def SGD(model, training_data, training_labels, validation_data, validation_labels, num_epochs, learning_rate, embed = False):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    model_optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for i in tqdm(range(num_epochs), desc = \"TRAINING MODEL\"):\n",
    "        total_train_loss = 0\n",
    "        for j in range(len(training_data)):\n",
    "            rand = np.random.randint(0, len(training_data))\n",
    "            \n",
    "            x = torch.unsqueeze(torch.tensor(training_data[rand], dtype = torch.float), 0)\n",
    "            if (embed == True):\n",
    "                x = torch.unsqueeze(torch.tensor(training_data[rand], dtype = torch.long), 0)\n",
    "                \n",
    "            y = torch.tensor(training_labels[rand], dtype = torch.float)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            train_loss = criterion(torch.squeeze(y_hat), y)\n",
    "            total_train_loss += train_loss\n",
    "            \n",
    "            train_loss.backward()\n",
    "            model_optimizer.step()\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "        \n",
    "        print(\"TRAINING LOSS AT EPOCH\", i + 1, \"IS:\", float(total_train_loss / len(training_data)))\n",
    "        train_losses.append(float(total_train_loss / len(training_data)))\n",
    "        \n",
    "        valid_loss = 0\n",
    "        valid_x = torch.tensor(validation_data, dtype = torch.float)\n",
    "        if (embed == True):\n",
    "            valid_x = torch.tensor(validation_data, dtype = torch.long)\n",
    "        valid_y = torch.tensor(valid_labels, dtype = torch.float)\n",
    "        valid_y_hat = model(valid_x)\n",
    "        valid_loss = criterion(valid_y_hat, valid_y)\n",
    "        \n",
    "        print(\"VALIDATION LOSS AT EPOCH\", i + 1, \"IS:\", float(valid_loss), '\\n')\n",
    "        valid_losses.append(float(valid_loss))\n",
    "        \n",
    "    \n",
    "    return train_losses, valid_losses\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING THE LOGISTIC REGRESSION MODEL:\n",
    "logreg_train_losses, logreg_valid_losses = SGD(LogReg, train_set, train_labels, valid_set, valid_labels, num_epochs_logreg, learning_rate_logreg, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a94020",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "\n",
    "for num in range(num_epochs_logreg):\n",
    "    epochs.append(num)\n",
    "\n",
    "plt.plot(epochs, logreg_train_losses)\n",
    "plt.plot(logreg_valid_losses)\n",
    "plt.legend(['TRAINING LOSSES', 'VALIDATION LOSSES'])\n",
    "plt.title(\"LOSS PER EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATING THE MODEL WITH THE TEST SET\n",
    "# INPUT: model (type of model), test_data (testing data (features or embeddings)), test_labels (0 and 1)\n",
    "# model_name (name of model), embed (boolean value that converts embedding indices into proper type)\n",
    "def Test(model, test_data, test_labels, model_name, embed = False):\n",
    "    x = torch.tensor(test_data, dtype = torch.float)\n",
    "    if (embed == True):\n",
    "        x = torch.tensor(test_data, dtype = torch.long)\n",
    "    y_hat = model(x)\n",
    "    y_hat = list([int(i.round()) for i in y_hat])\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_labels, y_hat)\n",
    "    precision = metrics.precision_score(test_labels, y_hat)\n",
    "    recall = metrics.recall_score(test_labels, y_hat)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_labels, y_hat)\n",
    "    print(\"ACCURACY SCORE FOR\", model_name, \"IS:\", accuracy)\n",
    "    print(\"PRECISION SCORE FOR\", model_name, \"IS:\", precision)\n",
    "    print(\"RECALL SCORE FOR\", model_name, \"IS:\", recall, '\\n')\n",
    "    print(\"CONFUSION MATRIX:\", '\\n', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ff191",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test(LogReg, test_set, test_labels, \"LOGISTIC REGRESSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CNN WITH TF-IDF AND ENGINEERED FEATURES\n",
    "cnn_train_losses, cnn_valid_losses = SGD(CNN, train_set, train_labels, valid_set, valid_labels, num_epochs_cnn, learning_rate_cnn, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7111313",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "\n",
    "for num in range(num_epochs_cnn):\n",
    "    epochs.append(num)\n",
    "\n",
    "plt.plot(epochs, cnn_train_losses)\n",
    "plt.plot(cnn_valid_losses)\n",
    "plt.legend(['TRAINING LOSSES', 'VALIDATION LOSSES'])\n",
    "plt.title(\"LOSS PER EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test(CNN, test_set, test_labels, \"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013700b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CNN WITH WORD EMBEDDINGS:\n",
    "cnn_e_train_losses, cnn_e_valid_losses = SGD(CNN_E, train_indices, train_labels, valid_indices, valid_labels, num_epochs_cnn_e, learning_rate_cnn_e, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ec967",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "\n",
    "for num in range(num_epochs_cnn_e):\n",
    "    epochs.append(num)\n",
    "\n",
    "plt.plot(epochs, cnn_e_train_losses)\n",
    "plt.plot(cnn_e_valid_losses)\n",
    "plt.legend(['TRAINING LOSSES', 'VALIDATION LOSSES'])\n",
    "plt.title(\"LOSS PER EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ffe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test(CNN_E, test_indices, test_labels, \"CNN WITH EMBEDDINGS\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee80a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
